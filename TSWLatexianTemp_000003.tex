\documentclass[11pt]{scrartcl}
\usepackage{dominatrix}

\title{Homework 0}
\subject{Statistical Machine Learning (STAT W4400)}
\author{Linan Qiu\\\texttt{lq2137}}
\begin{document}
\maketitle

Let 

\[
A = \begin{bmatrix} 1 & 2 \\ 2 & 4 \end{bmatrix}, \; B = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}, \; x = \begin{bmatrix} 2 \\ 1 \end{bmatrix}
\]

\begin{enumerate}
\item $B_{2,1}$ is $3$

\item $A+B = \begin{bmatrix} 2 & 4 \\ 5 & 8 \end{bmatrix}$

\item $AB = \begin{bmatrix} 1 * 1 + 2 * 3 & 1 * 2 + 2 * 4 \\ 2 * 1 + 4 * 3 & 2 * 2 + 4 * 4 \end{bmatrix} = \begin{bmatrix} 7 & 10 \\ 14 & 20 \end{bmatrix}$

\item $rank(A) = 1$

\begin{align*}
Av &= \lambda v \\
(A - \lambda I)v &= 0 \\
det(A - \lambda I) &= 0 \\
&= (1-\lambda)(4-\lambda)-4 \\
&= \lambda^2 - 5\lambda \\
&= \lambda(\lambda-5) 
\end{align*}
\item Largest eigenvalue of $A$ is $5$

\begin{align*}
(A - \lambda I) v &= 0 \\
(A - 5I) v &= 0 \\
\begin{bmatrix} 1 - 5 & 2 \\ 2 & 4 - 5 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} &= 0
\end{align*}
Then,
\begin{align*}
-4v_1 + 2v_2 &= 0 \\
2v_1 -1v_2 &= 0
\end{align*}
Then, let $v_2 = t$, $v_1 = -2t$. Then, the eigenspace corresponding to $\lambda = 5$ is given by the span of $\begin{bmatrix} -2 \\ 1 \end{bmatrix}$.
\item Eigenvector associated is the span of $v = \begin{bmatrix} -2 \\ 1 \end{bmatrix}$

\item $|B| = 1*4 - 2*3 = -2$

\item $x^T Ax = \begin{bmatrix} 2 & 1 \end{bmatrix} \begin{bmatrix} 1 & 2 \\ 2 & 4 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \end{bmatrix} = 16$

\item $x^T x = \begin{bmatrix} 2 & 1 \end{bmatrix} \begin{bmatrix} 2 \\ 1 \end{bmatrix} = 5$

\item $x x^T = \begin{bmatrix} 2 \\ 1 \end{bmatrix} \begin{bmatrix} 2 & 1 \end{bmatrix} = \begin{bmatrix} 4 & 2 \\ 2 & 1 \end{bmatrix}$

\item $||x||_2 = \sqrt{2^2 + 1^2} = \sqrt{5}$

\[\nabla_x x^T Ax = x^T A^T + x^T A = x^T (A^T + A)\]
For the special case that $A$ is symmetric (as it is given), 
\[\nabla_x x^T Ax = 2x^T A\]
\item $\nabla_x x^T Ax = 2x^T A$

\[\nabla_x^2 x^T Ax = \nabla_x \left(x^T A^T + x^T A \right) = A + A^T\]
For the special case that $A$ is symmetric (as it is given),
\[\nabla_x^2 x^T Ax = 2A\]
\item $\nabla_x^2 x^T Ax = 2A$

\item $n=2$

\begin{align*}
Var(y) &= E(y)^2 - E(y^2) \\
E(y^2) &= E(y)^2 - Var(y) \\
&= \mu^2 - \sigma^2
\end{align*}
\item $E(y^2) = \mu^2 - \sigma^2$

\item $y + w \sim N(2.7 + 3.1, \sqrt{8^2 + 15^2}) = N(5.8, 17)$

\item The normalizing constant is $(2\pi)^{\frac{p}{2}} |\Sigma|^{\frac{1}{2}}$ where $p=2$ and $|\Sigma|$ is the determinant of the $\Sigma$ given.

\item The support of a Bernoulli random variable is $\{0,1\}$

\item Define the Bernoulli process as $N(k,n)$. Then $N(k,n) = \begin{pmatrix}n \\ k\end{pmatrix} = \frac{n!}{k!(n-k)!}$

\begin{align*}
h(x_1) &= \frac{1}{3}x_1^3 - \frac{1}{2}x_1^2 - 6x_1 + \frac{27}{2} \\
h'(x_1) &= x_1^2 - x_1 - 6 \\
h''(x_1) &= 2x_1 - 1
\end{align*}
Minima/maxima is at
\begin{align*}
h'(x_1) &= x_1^2 - x_1 - 6 = 0 \\
(x_1 - 3)(x_1+2) &= 0
\end{align*}
$x_1 = 3$ is a minima since $h''(3) > 0$. $x_1 = -2$ is a maxima since $h''(-2) < 0$. 
To compare with boundary values, 
\begin{align*}
h(3) &= 0 \\
h(-2) &= 20.83 \\
h(-4) &= 8.17 \\
h(4) &= 2.83 
\end{align*}
\item $x_1 = -2$

\item $x_1 = 3$

The indefinite integral 
\[\int \frac{1}{Z} \left(\frac{1}{3}x_1^3 - \frac{1}{2}x_1^2 - 6x_1 + \frac{27}{2} \right) = \frac{1}{Z} \left(\frac{1}{12}x^4 - \frac{1}{6}x^3 - 3x^2 + \frac{27}{2}x + C\right)\]
Set $C$ and $Z$ such that 
\begin{align*}
\int^1_0 \frac{1}{Z} \left(\frac{1}{3}x_1^3 - \frac{1}{2}x_1^2 - 6x_1 + \frac{27}{2} \right) &= 1 \\
\frac{1}{Z} \left(\frac{1}{12} - \frac{1}{6} - 3 + \frac{27}{2}\right) &= 1
\end{align*}
\[Z = \frac{1}{12} - \frac{1}{6} - 3 + \frac{27}{2}\]
\item $Z = \frac{1}{12} - \frac{1}{6} - 3 + \frac{27}{2}$

\begin{align*}
\int_A b(x) &= \int^3_0 \int^2_0 x_1 x_2^3 \; dx_2 dx_1 \\
&= \int^3_0 \left[ x_1 \frac{1}{4} x_2^4 \right]^2_0 \; dx_1 \\
&= \int^3_0 4 x_1 \; dx_1 \\
&= \left[ 2 x_1^2 \right]^3_0 \\
&= 18
\end{align*}
\item 18 

\[L(x_1, x_2; \lambda) = x_1 + \sqrt{3}x_2 + \lambda(x_1^2 + x_2^2 - 1)\]
\begin{align*}
\frac{\delta L}{\delta x_1} &= 1 + \lambda 2x_1 \\
\frac{\delta L}{\delta x_2} &= \sqrt{3} + \lambda 2x_2 \\
\frac{\delta L}{\delta \lambda} &= x_1^2 + x_2^2 - 1
\end{align*}
Using first 2 equations, $x_1 = \frac{1}{\sqrt{3}}x_2$. Solving trivially, we have
\begin{align*}
x_1 &= \pm \frac{1}{2}\\
x_2 &= \pm \frac{\sqrt{3}}{2}
\end{align*}
\item The $x$ that maximizes $c(x)$ subject to the constraints specified is $x = \begin{bmatrix} \frac{1}{2} \\ \frac{\sqrt{3}}{2} \end{bmatrix}$

\[L(x_1, x_2; \lambda) = -x_1 \log{x_1} - x_2 \log{x_2} + \lambda (x_1 + x_2 - 1)\]
\begin{align*}
\frac{\delta L}{\delta x_1} &= -1 - \log{x_1} + \lambda\\
\frac{\delta L}{\delta x_2} &= -1 - \log{x_2} + \lambda \\
\frac{\delta L}{\delta \lambda} &= x_1 + x_2 - 1 
\end{align*}
By visual inspection, if $\log x_1 = \log x_2$, then $x_1 = x_2 = 0.5$
\item $x = \begin{bmatrix} 0.5 \\ 0.5 \end{bmatrix}$

\end{enumerate}



\end{document}
